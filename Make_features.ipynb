{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297f19d-d20f-4516-a47c-6905e10e5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class Vacancy:\n",
    "    def __init__(self, uuid: str, name: str, context: str, keys: list):\n",
    "        self.uuid = uuid\n",
    "        self.name = name\n",
    "        self.context = name + '. ' + context\n",
    "        self.keys = keys\n",
    "        self.keys.append(name)\n",
    "        self.keys.extend(name.split(' '))\n",
    "        self.essense = []\n",
    "\n",
    "    def add_context(self, context: str | list[str]):\n",
    "        self.context = context\n",
    "\n",
    "    def add_keys(self, keys: list):\n",
    "        self.keys.extend(keys)\n",
    "\n",
    "    def add_key(self, key: str):\n",
    "        self.keys.append(key)\n",
    "\n",
    "    def add_essense(self, essense: list[str]):\n",
    "        self.essense.extend(essense)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        with open(path, 'wb') as fl:\n",
    "            pickle.dump(self, fl)\n",
    "\n",
    "\n",
    "class Resume:\n",
    "    def __init__(self, uuid: str, years: int, keys: list, experience_day,\n",
    "             experience, edu_keys, result=None):\n",
    "        self.uuid = uuid\n",
    "        self.years = years\n",
    "        self.keys = keys + edu_keys\n",
    "        self.experience_day = experience_day\n",
    "        self.experience = experience\n",
    "        self.target = result\n",
    "\n",
    "    def add_experience(self, new_exp: str):\n",
    "        self.experience.append(new_exp)\n",
    "\n",
    "    def add_keys(self, keys: list):\n",
    "        self.keys.extend(keys)\n",
    "\n",
    "    def add_key(self, key: str):\n",
    "        \n",
    "        self.keys.append(key)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str):\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f4e26-a156-45df-a714-182419c7fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import Vacancy, Resume\n",
    "\n",
    "from translatepy import Translator\n",
    "from translatepy.translators.google import GoogleTranslate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "def tranlate_str(text):\n",
    "    return translator.translate(text, \"English\").result\n",
    "\n",
    "\n",
    "def load_vacancy(data):\n",
    "    keys = []\n",
    "    if data['keywords'] is not None:\n",
    "        text = tranlate_str(data['keywords'])\n",
    "        keys = text.split(', ')\n",
    "\n",
    "    obj = Vacancy(data['uuid'], tranlate_str(data['name']), tranlate_str(data['description']),\n",
    "                  keys)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def load_resume(data, result=None):\n",
    "    worked_days = []\n",
    "    lst_exp = []\n",
    "    edu_keys = []\n",
    "\n",
    "    today = datetime.today()\n",
    "\n",
    "    if 'experienceItem' in data.keys():\n",
    "        experienceItem = data['experienceItem']\n",
    "\n",
    "        for i in experienceItem:\n",
    "            val = ''\n",
    "            if i['position'] is not None and i['position'].strip() != '':\n",
    "                val += tranlate_str(i['position'])\n",
    "            if i['description'] is not None and i['description'].strip() != '':\n",
    "                val += tranlate_str(i['description'])\n",
    "            lst_exp.append(val)\n",
    "            if i['ends'] is None:\n",
    "                worked_days.append((today - datetime.strptime(i['starts'], '%Y-%m-%d')).days)\n",
    "            else:\n",
    "                worked_days.append(\n",
    "                    (datetime.strptime(i['ends'], '%Y-%m-%d') - datetime.strptime(i['starts'], '%Y-%m-%d')).days)\n",
    "\n",
    "    if 'educationItem' in data.keys():\n",
    "        educationItem = data['educationItem']\n",
    "        for i in educationItem:\n",
    "            if i['result'] != '' and i['result'] is not None:\n",
    "                text = tranlate_str(i['result'])\n",
    "                edu_keys.append(text)\n",
    "                edu_keys.extend(text.split(' '))\n",
    "            elif i['specialty'] != '' and i['specialty'] is not None:\n",
    "                text = tranlate_str(i['specialty'])\n",
    "                edu_keys.append(text)\n",
    "                edu_keys.extend(text.split(' '))\n",
    "            elif i['faculty'] != '' and i['faculty'] is not None:\n",
    "                text = tranlate_str(i['faculty'])\n",
    "                edu_keys.append(text)\n",
    "                edu_keys.extend(text.split(' '))\n",
    "\n",
    "    date = 33\n",
    "    years = 0\n",
    "    if 'birth_date' in data.keys():\n",
    "        if data['birth_date'] is not None:\n",
    "            date = datetime.strptime(data['birth_date'], '%Y-%m-%d')\n",
    "            years = (today - date).days // 365\n",
    "\n",
    "    keys = []\n",
    "    if data['key_skills'] is not None:\n",
    "        text = tranlate_str(data['key_skills'])\n",
    "        keys.extend(text.split(', '))\n",
    "    obj = Resume(data['uuid'], years, keys,\n",
    "                 worked_days, lst_exp, edu_keys, result)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def load_train(way):\n",
    "    with open(way, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    vacancies = []\n",
    "    lst_failed_resumes = []\n",
    "    lst_confirmed_resumes = []\n",
    "    for j, i in enumerate(data):\n",
    "        vacancy, failed_resumes, confirmed_resumes = i['vacancy'], i['failed_resumes'], i['confirmed_resumes']\n",
    "        vacancies.append(load_vacancy(vacancy))\n",
    "        lst_failed = []\n",
    "        lst_confirmed = []\n",
    "        for resume in failed_resumes:\n",
    "            lst_failed.append(load_resume(resume, result=0))\n",
    "        for resume in confirmed_resumes:\n",
    "            lst_confirmed.append(load_resume(resume, result=1))\n",
    "        lst_failed_resumes.append(lst_failed)\n",
    "        lst_confirmed_resumes.append(lst_confirmed)\n",
    "\n",
    "    return vacancies, lst_failed_resumes, lst_confirmed_resumes\n",
    "\n",
    "\n",
    "def load_test(way):\n",
    "    with open(way, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "    vacancies = []\n",
    "    lst_resumes = []\n",
    "    vacancy, resumes = data['vacancy'], data['resumes']\n",
    "    print('start')\n",
    "    vacancies.append(load_vacancy(vacancy))\n",
    "    print('vacancy done')\n",
    "    lst = []\n",
    "    for resume in resumes:\n",
    "        lst.append(load_resume(resume))\n",
    "    lst_resumes.append(lst)\n",
    "    print('resume done')\n",
    "    return vacancies, lst_resumes\n",
    "\n",
    "\n",
    "way_test = 'test_data.json'\n",
    "test_vacancies, test_lst_resumes = load_test(way_test)\n",
    "print(\"add in class(translated)\")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"Voicelab/vlt5-base-keywords\", legacy=False)\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"Voicelab/vlt5-base-keywords\")\n",
    "\n",
    "with open('all_hard_skills.pkl', 'rb') as f:\n",
    "    skills = pickle.load(f)\n",
    "\n",
    "\n",
    "def generate_keywords(text):\n",
    "    task_prefix = \"Keywords: \"\n",
    "    inputs = text.split('.')\n",
    "    set_key_words = set()\n",
    "\n",
    "    for sample in inputs:\n",
    "        input_sequences = [task_prefix + sample]\n",
    "        input_ids = tokenizer2(\n",
    "            input_sequences, return_tensors=\"pt\", truncation=True\n",
    "        ).input_ids\n",
    "        output = model2.generate(input_ids, no_repeat_ngram_size=3, num_beams=4)\n",
    "        predicted = tokenizer2.decode(output[0], skip_special_tokens=True)\n",
    "        set_key_words.update(predicted.split(', '))\n",
    "\n",
    "    return list(set_key_words)\n",
    "\n",
    "\n",
    "for i in range(len(test_vacancies)):\n",
    "    desr = test_vacancies[i].context\n",
    "\n",
    "    skill = set()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', desr).replace('  ', ' ').lower()\n",
    "    for word in text.split(' '):\n",
    "        if word in skills:\n",
    "            skill.add(word)\n",
    "\n",
    "    test_vacancies[i].add_essense(list(skill))\n",
    "    test_vacancies[i].add_essense(generate_keywords(desr))\n",
    "\n",
    "print(\"saving\")\n",
    "\n",
    "with open(\"test_vacancies.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_vacancies, f)\n",
    "\n",
    "with open(\"test_lst_resumes.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_lst_resumes, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b639eb4-6564-4937-8f6f-6883bd118926",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf5ec4-18a6-4464-aad4-a6443c17207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigBinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)   # Первый полносвязный слой с 16 входами и 64 нейронами\n",
    "        self.fc2 = nn.Linear(64, 128)  # Второй полносвязный слой с 64 входами и 128 нейронами\n",
    "        self.fc3 = nn.Linear(128, 64)  # Третий полносвязный слой с 128 входами и 64 нейронами\n",
    "        self.fc4 = nn.Linear(64, 32)   # Четвертый полносвязный слой с 64 входами и 32 нейронами\n",
    "        self.fc5 = nn.Linear(32, 1)    # Пятый полносвязный слой с 32 входами и одним выходным нейроном\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем функции активации ReLU к выходу каждого слоя\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))  # Применяем сигмоидальную функцию к выходу пятого слоя\n",
    "        return x\n",
    "\n",
    "# Пример использования модели\n",
    "model = BigBinaryClassifier()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bfd63-5b36-4c7a-a61f-ba585b321c97",
   "metadata": {},
   "source": [
    "# Make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0f35e-e331-4adc-9906-ca6c2b2d6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import Vacancy, Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f0d7f-d248-499a-9941-66b2eb292779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель для сравнения сущностей\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "feature_param = 5\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6af9a-d899-4347-96c4-c5bd6427d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similar(sentence1, sentence2, model): ## сравнение текста -> int\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "    # Вычисление косинусного расстояния между векторами\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    \n",
    "    return cosine_scores.item()\n",
    "\n",
    "def calc_hard_skills_sim(person_skills, vac_skills, param): ## -> сравнение важных скилов list(int)\n",
    "    # Initialize a matrix to store similarity scores between all pairs of person and vacancy skills\n",
    "    matrix = []\n",
    "    for v_skill in vac_skills:\n",
    "        feature = 1 if v_skill in person_skills else 0\n",
    "        matrix.append(feature)\n",
    "    #print(matrix)\n",
    "    if len(matrix) < param:\n",
    "        if len(matrix) / param <= 0.5:\n",
    "            return None\n",
    "        return matrix + [0] * (param - len(matrix))\n",
    "    return matrix[:param]\n",
    "\n",
    "def calc_essence_sim(person_skills, essence_vac, model, param):## -> сравнение важных сущностей list(int)\n",
    "    # Initialize a matrix to store similarity scores between all pairs of person and vacancy skills\n",
    "    matrix = []\n",
    "    for v_skill in essence_vac:\n",
    "        maxi = -1\n",
    "        for p_skill in person_skills:\n",
    "            # Calculate cosine similarity between each pair of person and vacancy skills\n",
    "            cos_score = text_similar(p_skill, v_skill, model)\n",
    "            maxi = max(maxi, cos_score)\n",
    "        matrix.append(maxi)\n",
    "    if len(matrix) < param:\n",
    "        return matrix + [0] * (param - len(matrix))\n",
    "    return matrix[:param]\n",
    "\n",
    "def rate_skills(name_vac, hard_skills_vac, model):## -> ранжирование списка list(str)\n",
    "    scores = []\n",
    "    for i in hard_skills_vac:\n",
    "        scores.append(text_similar(name_vac, i, model))\n",
    "    # Получение индексов максимальных элементов (пять первых)\n",
    "    max_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    imp_skiils = [hard_skills_vac[i] for i in max_indexes]\n",
    "    return imp_skiils\n",
    "\n",
    "def experiance_disсribe_vac(lst_exp_desc, time_exp, discribe_vac, embending_model, param):  \n",
    "    '''\n",
    "    lst_exp_desc - список с описанием вакансий,\n",
    "    time_exp - время работы на определенной вакансии\n",
    "    discribe_vac - описание вакансии\n",
    "    embending_model - модель для емб\n",
    "    return lst[int]\n",
    "    '''\n",
    "    scores = []\n",
    "    for exp, time in zip(lst_exp_desc, time_exp):\n",
    "        scores.append(text_similar(exp, discribe_vac, embending_model) * time / 365)\n",
    "    scores.sort(reverse=True)\n",
    "    if len(scores) < param:\n",
    "        return scores + [0] * (param - len(scores))\n",
    "    return scores[:param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b287e66-656f-4845-a42a-39bf58b36306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#/Users/igorkopylov/Downloads/essence.pkl\n",
    "with open(\"/Users/igorkopylov/Downloads/train_lst_confirmed_resumes.pkl\", \"rb\") as f:\n",
    "    confirmed_resumes = pickle.load(f, encoding = 'utf8')\n",
    "\n",
    "with open(\"/Users/igorkopylov/Downloads/train_lst_failed_resumes.pkl\", \"rb\") as f:\n",
    "    failed_resumes = pickle.load(f, encoding = 'utf8')\n",
    "\n",
    "with open(\"/Users/igorkopylov/Downloads/train_vacancies.pkl\", \"rb\") as f:\n",
    "    vacancies = pickle.load(f, encoding = 'utf8')\n",
    "\n",
    "\n",
    "for i in range(len(vacancies)):\n",
    "    resumes.append(confirmed_resumes[i] + failed_resumes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51c746-570f-4f7a-a00f-fd41194c4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter  # Import SummaryWriter for TensorBoard logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create a SummaryWriter instance\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa506e0-5eba-4835-90af-1c4913110966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(lst_objects, embedding_model, feature_param):\n",
    "    # Создание пустого списка для тензор\n",
    "    data = []\n",
    "    target = []\n",
    "    for indx, vac in enumerate(lst_objects[1]):\n",
    "        discribe_vac = vac.context\n",
    "        name_vac = vac.name\n",
    "        essence_vac = vac.essense\n",
    "        hard_skills_vac = vac.keys\n",
    "        for res in lst_objects[0][indx]:\n",
    "            lst_exp_desc = res.experience\n",
    "            hard_skills_per = res.keys\n",
    "            exp_day = res.experience_day\n",
    "            age = res.years\n",
    "            feature = []\n",
    "            # try:\n",
    "            # сравнение hard_skills_per с essence_vac\n",
    "            imp_essence = rate_skills(name_vac, essence_vac, embedding_model)\n",
    "            essence_emb = calc_essence_sim(hard_skills_per, imp_essence, embedding_model, feature_param)\n",
    "            # сравнение hard_skills_per с essence_vac\n",
    "            imp_skills_vac = rate_skills(name_vac, hard_skills_vac, embedding_model)\n",
    "            skills_emb = calc_hard_skills_sim(hard_skills_per, imp_skills_vac, feature_param)\n",
    "            if skills_emb is None:\n",
    "                skills_emb = essence_emb\n",
    "            # Сравнение рабочего опыта резюмиста и описание вакансии\n",
    "            exp_emb = experiance_disсribe_vac(lst_exp_desc, exp_day, discribe_vac, embedding_model, feature_param)\n",
    "            # lst_exp_desc, time_exp, discribe_vac, embending_model, param\n",
    "            feature.extend(skills_emb + essence_emb + exp_emb + [age])\n",
    "            print(feature)\n",
    "            #except:\n",
    "                #feature = [0] * (feature_param * 3)\n",
    "            data.append(feature)\n",
    "            target.append([res.target])\n",
    "        print(indx)\n",
    "    return data, target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd6cd9-65db-4cb6-8eda-4f6cfdd7cdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_features((resumes, vacancies), embedding_model, feature_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705017f2-1bc3-49ea-834b-846002a2ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fail_feature.csv')\n",
    "conf = pd.read_csv('conf.csv')\n",
    "print(conf.shape)\n",
    "df['target'] = 0\n",
    "conf['target'] = 1\n",
    "#df['target']\n",
    "df['target'] = df['target'].astype(float)\n",
    "print(df.shape)\n",
    "\n",
    "df = pd.concat([df, conf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec948a5-ee64-497f-9e5b-785494707f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"target\", axis=1), df[\"target\"], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(X_train.shape, y_train.shape)#, dtype=torch.float32)\n",
    "#print(X_train.shape, X_test.shape, dtype=torch.float32)\n",
    "#Создание DataLoader для обучения и валидации\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_test, y_test)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25a874e8-27fd-4de8-9b7d-fba82051ef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigBinaryClassifier(\n",
      "  (fc1): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BigBinaryClassifier()\n",
    "print(model)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e83d7ba4-a21e-46a2-8aae-b225707c89f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 0.6718, Val Loss: 0.6400, Val Accuracy: 0.7259\n",
      "Epoch 2/40, Train Loss: 0.6371, Val Loss: 0.6105, Val Accuracy: 0.7259\n",
      "Epoch 3/40, Train Loss: 0.6216, Val Loss: 0.5970, Val Accuracy: 0.7259\n",
      "Epoch 4/40, Train Loss: 0.6162, Val Loss: 0.5923, Val Accuracy: 0.7259\n",
      "Epoch 5/40, Train Loss: 0.6157, Val Loss: 0.5899, Val Accuracy: 0.7259\n",
      "Epoch 6/40, Train Loss: 0.6158, Val Loss: 0.5900, Val Accuracy: 0.7259\n",
      "Epoch 7/40, Train Loss: 0.6159, Val Loss: 0.5903, Val Accuracy: 0.7259\n",
      "Epoch 8/40, Train Loss: 0.6157, Val Loss: 0.5891, Val Accuracy: 0.7259\n",
      "Epoch 9/40, Train Loss: 0.6153, Val Loss: 0.5911, Val Accuracy: 0.7259\n",
      "Epoch 10/40, Train Loss: 0.6159, Val Loss: 0.5919, Val Accuracy: 0.7259\n",
      "Epoch 11/40, Train Loss: 0.6146, Val Loss: 0.5894, Val Accuracy: 0.7259\n",
      "Epoch 12/40, Train Loss: 0.6148, Val Loss: 0.5889, Val Accuracy: 0.7259\n",
      "Epoch 13/40, Train Loss: 0.6148, Val Loss: 0.5896, Val Accuracy: 0.7259\n",
      "Epoch 14/40, Train Loss: 0.6145, Val Loss: 0.5905, Val Accuracy: 0.7259\n",
      "Epoch 15/40, Train Loss: 0.6144, Val Loss: 0.5903, Val Accuracy: 0.7259\n",
      "Epoch 16/40, Train Loss: 0.6142, Val Loss: 0.5900, Val Accuracy: 0.7259\n",
      "Epoch 17/40, Train Loss: 0.6142, Val Loss: 0.5892, Val Accuracy: 0.7259\n",
      "Epoch 18/40, Train Loss: 0.6143, Val Loss: 0.5903, Val Accuracy: 0.7259\n",
      "Epoch 19/40, Train Loss: 0.6145, Val Loss: 0.5910, Val Accuracy: 0.7259\n",
      "Epoch 20/40, Train Loss: 0.6140, Val Loss: 0.5899, Val Accuracy: 0.7259\n",
      "Epoch 21/40, Train Loss: 0.6145, Val Loss: 0.5877, Val Accuracy: 0.7259\n",
      "Epoch 22/40, Train Loss: 0.6137, Val Loss: 0.5899, Val Accuracy: 0.7259\n",
      "Epoch 23/40, Train Loss: 0.6135, Val Loss: 0.5905, Val Accuracy: 0.7259\n",
      "Epoch 24/40, Train Loss: 0.6141, Val Loss: 0.5886, Val Accuracy: 0.7259\n",
      "Epoch 25/40, Train Loss: 0.6135, Val Loss: 0.5906, Val Accuracy: 0.7259\n",
      "Epoch 26/40, Train Loss: 0.6141, Val Loss: 0.5894, Val Accuracy: 0.7259\n",
      "Epoch 27/40, Train Loss: 0.6131, Val Loss: 0.5903, Val Accuracy: 0.7259\n",
      "Epoch 28/40, Train Loss: 0.6140, Val Loss: 0.5913, Val Accuracy: 0.7259\n",
      "Epoch 29/40, Train Loss: 0.6126, Val Loss: 0.5882, Val Accuracy: 0.7333\n",
      "Epoch 30/40, Train Loss: 0.6133, Val Loss: 0.5881, Val Accuracy: 0.7333\n",
      "Epoch 31/40, Train Loss: 0.6125, Val Loss: 0.5889, Val Accuracy: 0.7333\n",
      "Epoch 32/40, Train Loss: 0.6122, Val Loss: 0.5900, Val Accuracy: 0.7333\n",
      "Epoch 33/40, Train Loss: 0.6120, Val Loss: 0.5899, Val Accuracy: 0.7333\n",
      "Epoch 34/40, Train Loss: 0.6118, Val Loss: 0.5891, Val Accuracy: 0.7333\n",
      "Epoch 35/40, Train Loss: 0.6121, Val Loss: 0.5899, Val Accuracy: 0.7333\n",
      "Epoch 36/40, Train Loss: 0.6113, Val Loss: 0.5891, Val Accuracy: 0.7333\n",
      "Epoch 37/40, Train Loss: 0.6112, Val Loss: 0.5892, Val Accuracy: 0.7333\n",
      "Epoch 38/40, Train Loss: 0.6112, Val Loss: 0.5887, Val Accuracy: 0.7333\n",
      "Epoch 39/40, Train Loss: 0.6116, Val Loss: 0.5891, Val Accuracy: 0.7333\n",
      "Epoch 40/40, Train Loss: 0.6105, Val Loss: 0.5904, Val Accuracy: 0.7333\n",
      "Обучение завершено.\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Установка модели в режим обучения\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.squeeze()\n",
    "        #print(inputs.shape)\n",
    "        optimizer.zero_grad()  # Обнуление градиентов\n",
    "        #print(inputs.dtype, targets.dtype)\n",
    "        outputs = model(inputs).squeeze()  # Прямой проход\n",
    "        loss = criterion(outputs, targets)  # Вычисление функции потерь\n",
    "        loss.backward()  # Обратное распространение\n",
    "        optimizer.step()  # Обновление параметров\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Валидация модели\n",
    "    model.eval()  # Установка модели в режим оценки\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs\n",
    "            outputs = model(inputs).squeeze()#.squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            predicted = torch.round(outputs)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    # Запись в TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print('Обучение завершено.')\n",
    "\n",
    "# Закрытие SummaryWriter после завершения обучения\n",
    "writer.close() # [1, 32, 16] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca9469-0c7e-48da-bb53-30d2fb7b508c",
   "metadata": {},
   "outputs": [],
   "source": [
    " %load_ext tensorboard\n",
    " %tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58c4d78b-4b82-463d-8559-9ca8fa25be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_Big.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b056f9bb-d5ac-4810-b269-1fbae37bed10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"test_lst_resumes.pkl\", \"rb\") as f:\n",
    "    resumes = pickle.load(f, encoding = 'utf8')\n",
    "\n",
    "with open(\"test_vacancies.pkl\", \"rb\") as f:\n",
    "    vacancies = pickle.load(f, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "039fff12-ef10-4003-b94a-7e029455ec1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0.5625385642051697, 0.6015891432762146, 0.636419415473938, 0.4720681607723236, 0.3762739300727844, 0.9434269696065825, 0.7756954114731044, 0.7622999227210266, 0.16681434136547454, 0.07727329396221735, 32]\n",
      "[0, 0, 0, 0, 0, 0.585588276386261, 0.6995006203651428, 0.7412429451942444, 0.750575065612793, 0.48060208559036255, 2.7349927660537094, 1.0866385348855632, 0.8867967448822439, 0.5692268995389547, 0, 33]\n",
      "[0, 0, 0, 0, 0, 0.5319784283638, 0.6450060606002808, 0.7060478925704956, 0.8253291845321655, 0.40796905755996704, 1.3799522256197996, 0.7214927385931146, 0.40830624626107415, 0, 0, 34]\n",
      "[0, 0, 1, 1, 0, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.42550888657569885, 1.898953263808603, 0.8865201892918103, 0.7409509951121187, 0.5991556210060642, 0.37904770308977936, 34]\n",
      "[0, 0, 0, 0, 0, 0.16846641898155212, 0.18248777091503143, 0.27730822563171387, 0.16293789446353912, 0.32951778173446655, 1.3034175513541861, 1.2485530889197571, 1.1183346157204614, 0.5561129974992308, 0.4881740849312038, 29]\n",
      "[0, 0, 1, 1, 1, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.4076592028141022, 0.8405517339706421, 0.4682215396672079, 0.09481388444769873, 0.06232110996768899, 0, 32]\n",
      "[0, 0, 0, 0, 0, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.7002347707748413, 1.8648020055196057, 1.012375494062084, 0.4731082756225377, 0.42679402730236315, 0, 32]\n",
      "[0, 0, 0, 0, 0, 0.32884740829467773, 0.44492197036743164, 0.4379139244556427, 0.33928826451301575, 0.27406176924705505, 1.6702996828784682, 0.85397968569847, 0.6796927870136418, 0, 0, 29]\n",
      "[0, 0, 1, 1, 0, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.48060208559036255, 1.2423680811712186, 0.7425835550647892, 0.6959784808224194, 0, 0, 37]\n",
      "[0, 0, 1, 1, 0, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.7106238603591919, 1.698753518845937, 0.671024645844551, 0.5811801231887243, 0, 0, 24]\n",
      "[0, 0, 1, 1, 0, 0.6349282264709473, 0.7856550812721252, 0.9999999403953552, 0.698638916015625, 0.46763888001441956, 1.2380311547893368, 1.079392314283815, 0.5269568162421657, 0.5085390273838827, 0, 28]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X, y = make_features((resumes, vacancies), embedding_model, feature_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ba9afc7-a682-45c2-a7b5-97f0d0794f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = torch.tensor(X, dtype=torch.float32)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64fe2f0a-f5d3-4033-a170-1d878a3798fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Предсказание на тестовых данных\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(outputs)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_labels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "model = torch.load('BigBinary.pth')\n",
    "\n",
    "# Преобразование данных в тензор\n",
    "X_test = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted_labels = torch.round(outputs).squeeze().tolist()\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1fe13-086d-4408-ae26-5aa9ca1fd694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
